---
title: "STAT/MATH 495: Problem Set 06"
author: "Leonard Yoon"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
```

# Collaboration

Please indicate who you collaborated with on this assignment: Tim, Wayne, Tasheena, Vickie

# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
```


# Computation

I will generate $n\_sample = 10,000$ samples of size $n = 500$. For each sample, I fit the two splines and predict $\widehat{y}$ for each spline model at $x_0 = .95$.

```{r, warning=FALSE, message=FALSE}
lm_pred <- runif(n=n_sample) # initialize vectors for y-hats
sp_pred <- runif(n=n_sample)

for (i in 1:n_sample) {
  samp <- generate_sample(f, n, sigma) # generate sample
  linear_model <- smooth.spline(x=samp$x, y=samp$y, df = 2) # essentially lm
  splines_model <- smooth.spline(x=samp$x, y=samp$y, df = 99) # df = 99 spline
  
  lm_pred[i] <- predict(linear_model,test_set$x)$y # test y-hat created from predict()
  sp_pred[i] <- predict(splines_model,test_set$x)$y 
}
```

I then calculate the MSE for each model. To do so, I will generate $n\_sample = 10,000$ values of $y$.

```{r, warning=FALSE, message=FALSE}
epsilon <- rnorm(n = n_sample, mean = 0, sd = sigma)
y <- f(.95) + epsilon # generate y's

MSElm <- (1/n_sample)*sum((y - lm_pred)^2) # calculate MSE
MSEsp <- (1/n_sample)*sum((y - sp_pred)^2)
```

Next, I calculate the Variance and Bias$^2$ for each model.

```{r, warning=FALSE, message=FALSE}
varlm <- var(lm_pred) # calculate variance
varsp <- var(sp_pred)

bias_sqlm <- (mean(lm_pred) - f(.95))^2 # calculate Bias^2
bias_sqsp <- (mean(sp_pred) - f(.95))^2
```

# Tables

```{r, warning=FALSE, message=FALSE}
dflm <- data_frame(MSE = MSElm, 
                   bias_squared = bias_sqlm, 
                   var = varlm, 
                   irreducible = sigma^2, # units need to be squared, so it's sigma^2
                   sum = bias_sqlm + varlm + sigma^2)

dfsp <- data_frame(MSE = MSEsp, 
                   bias_squared = bias_sqsp, 
                   var = varsp, 
                   irreducible = sigma^2, 
                   sum = bias_sqsp + varsp + sigma^2)

df <- rbind(dflm, dfsp) # output table compares km to spline df=99
rownames(df)=c("lm", "spline df=99")

df %>% knitr::kable(digits=4)
```

```{r, include=FALSE}
pct.err.lm <- (1 - ((bias_sqlm + varlm + sigma^2)/MSElm))*100
pct.err.sp <- (1 - ((bias_sqsp + varsp + sigma^2)/MSEsp))*100
```

# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
1. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
1. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. The sanity check is that there should be high bias and low variance for the df = 2 results and there should be low bias and high variance for the df = 99 results. Also, my percent error between my sum and my MSE for the lm is `r pct.err.lm`% and my percent error between my sum and my MSE for the spline with df = 99 is `r pct.err.sp`%, neither of which are very alarming.
1. The procedure would be very similar to what is done above, except the `predict` function would be done on the entire sample as opposed to just $x_0 = .95$. With 10,000 samples, we would be able to essentially predict points for the entire domain and thus calculate MSE for $\widehat{f}(x)$ as opposed to just the MSE for $\widehat{f}(.95)$.
1. The MSE is slightly lower for the df = 2 model from my simulation, so knowing that, I would prefer that model because it has a lower total error. I would also choose the underfit (linear) model for predicting the point of interest because despite the high bias, the low variance ensures that the error will be more consistent between the predicted point and the true value of the point of interest. This is preferred to having a high variance observation, which could potentially be very off the true value. 
